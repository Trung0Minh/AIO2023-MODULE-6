{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trung0Minh/AIO2023-MODULE-6/blob/main/visual_question_answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import spacy"
      ],
      "metadata": {
        "id": "FqUSdheN9_LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.15.2\n",
        "!pip install torch==1.13.0\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "ctfFjgdV-sd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset_path):\n",
        "    data = []\n",
        "    with open(dataset_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            temp = line.split(\"\\t\")\n",
        "            qa = temp[1].split('?')\n",
        "\n",
        "            if len(qa) == 3:\n",
        "                answer = qa[2].strip()\n",
        "            else:\n",
        "                answer = qa[1].strip()\n",
        "\n",
        "            data_sample = {\n",
        "                'image_path': temp[0][:-2],\n",
        "                'question': qa[0],\n",
        "                'answer': answer\n",
        "            }\n",
        "            data.append(data_sample)\n",
        "    return data\n",
        "\n",
        "train_set_path = './vaq2.0.TrainImages.txt'\n",
        "train_data = load_data(train_set_path)\n",
        "\n",
        "val_set_path = './vaq2.0.DevImages.txt'\n",
        "val_data = load_data(val_set_path)\n",
        "\n",
        "test_set_path = './vaq2.0.TestImages.txt'\n",
        "test_data = load_data(test_set_path)"
      ],
      "metadata": {
        "id": "eBP097fIASot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng = spacy.load(\"en_core_web_sm\")  # Load the English model to tokenize English text\n",
        "\n",
        "def get_tokens(data_iter):\n",
        "    for sample in data_iter:\n",
        "        question = sample[\"question\"]\n",
        "        yield [token.text for token in eng.tokenizer(question)]\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    get_tokens(train_data),\n",
        "    min_freq=2,\n",
        "    specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"],\n",
        "    special_first=True\n",
        ")\n",
        "\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "kARekXCLGkgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = set([sample['answer'] for sample in train_data])\n",
        "classes_to_idx = {\n",
        "    class_name: idx for idx, class_name in enumerate(classes)\n",
        "}\n",
        "\n",
        "idx_to_classes = {\n",
        "    idx: class_name for class_name, idx in enumerate(classes)\n",
        "}"
      ],
      "metadata": {
        "id": "dYY3EZoEG8d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(quesion, max_sequence_length):\n",
        "    tokens = [token.text for token in eng.tokenizer(quesion)]\n",
        "    sequence = [vocab[token] for token in tokens]\n",
        "    if len(sequence) < max_sequence_length:\n",
        "        sequence += [vocab[\"<pad>\"]] * (max_sequence_length - len(sequence))\n",
        "    else:\n",
        "        sequence = sequence[:max_sequence_length]\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "lZYvBUvyHtnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(self, data, classes_to_idx, max_seq_len=30, transform=None, root_dir='val2014-resised'):\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.root_dir = root_dir\n",
        "        self.classes_to_idx = classes_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data[idx]['image_path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        question = self.data[idx]['question']\n",
        "        question = tokenize(question, self.max_seq_len)\n",
        "        question = torch.tensor(question, dtype=torch.long)\n",
        "\n",
        "        answer = self.data[idx]['answer']\n",
        "        answer = self.classes_to_idx[answer]\n",
        "        answer = torch.tensor(answer, dtype=torch.long)\n",
        "\n",
        "        return image, question, answer"
      ],
      "metadata": {
        "id": "p7mLjQ_-H_IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "EVYFUiIIJ-Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = VQADataset(train_data, classes_to_idx, transform=transform)\n",
        "val_dataset = VQADataset(val_data, classes_to_idx, transform=transform)\n",
        "test_dataset = VQADataset(test_data, classes_to_idx, transform=transform)"
      ],
      "metadata": {
        "id": "Id03JppPKGDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 128\n",
        "test_batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "UPI8_P0IKI2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, n_classes, img_model_name='resnet50', embedding_dim=300, n_layers=2, hidden_size=128, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.image_encoder = timm.create_model(\n",
        "            img_model_name, pretrained=True, num_classes=hidden_size\n",
        "        )\n",
        "\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(hidden_size*2)\n",
        "        self.fc1 = nn.Linear(hidden_size*3, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(256, n_classes)\n",
        "\n",
        "    def forward(self, img, text):\n",
        "            img_features = self.image_encoder(img)\n",
        "\n",
        "            text_emb = self.embedding(text)\n",
        "            lstm_out, _ = self.lstm(text_emb)\n",
        "\n",
        "            lstm_out = lstm_out[:, -1, :]\n",
        "            lstm_out = self.layernorm(lstm_out)\n",
        "\n",
        "            combined = torch.cat((img_features, lstm_out), dim=1)\n",
        "\n",
        "            x = self.fc1(combined)\n",
        "            x = self.relu(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.fc2(x)\n",
        "            return x"
      ],
      "metadata": {
        "id": "_8JdDsdiKRYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(classes)\n",
        "img_model_name = \"resnet50\"\n",
        "hidden_size = 128\n",
        "n_layers = 1\n",
        "embedding_dim = 128\n",
        "dropout = 0.2\n",
        "device = 'cude' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = VQAModel(n_classes, img_model_name, embedding_dim, n_layers, hidden_size, dropout).to(device)"
      ],
      "metadata": {
        "id": "kzsjg85zRfxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    accs = []\n",
        "    with torch.no_grad():\n",
        "        for image, question, labels in dataloader:\n",
        "            image = image.to(device)\n",
        "            question = question.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(image, question)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    loss = sum(losses) / len(losses)\n",
        "    acc = correct / total\n",
        "    return loss, acc"
      ],
      "metadata": {
        "id": "heywGunCRxQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, train_dataloader, val_Dataloader, criterion, optimizer, scheduler, device, epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch_train_losses = []\n",
        "\n",
        "        for image, question, labels in train_dataloader:\n",
        "            image = image.to(device)\n",
        "            question = question.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(image, question)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "PxCbR4wiUDn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "epochs = 50\n",
        "weight_decay = 1e-5\n",
        "scheduler_step_size = epochs * 0.6\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=lr,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=scheduler_step_size,\n",
        "    gamma=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "2Ii3YxIcU3-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = fit(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, device, epochs)"
      ],
      "metadata": {
        "id": "gwbaNtvjU_hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_dataloader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "eqswwmVxVBYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vit + RoBERTa"
      ],
      "metadata": {
        "id": "DYtFNKpdVoQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "from transformers import AutoTokenizer, RobertaModel"
      ],
      "metadata": {
        "id": "kyTwVeVkVFbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset_path):\n",
        "    data = []\n",
        "    with open(dataset_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            temp = line.split(\"\\t\")\n",
        "            qa = temp[1].split('?')\n",
        "\n",
        "            if len(qa) == 3:\n",
        "                answer = qa[2].strip()\n",
        "            else:\n",
        "                answer = qa[1].strip()\n",
        "\n",
        "            data_sample = {\n",
        "                'image_path': temp[0][:-2],\n",
        "                'question': qa[0],\n",
        "                'answer': answer\n",
        "            }\n",
        "            data.append(data_sample)\n",
        "    return data\n",
        "\n",
        "train_set_path = './vaq2.0.TrainImages.txt'\n",
        "train_data = load_data(train_set_path)\n",
        "\n",
        "val_set_path = './vaq2.0.DevImages.txt'\n",
        "val_data = load_data(val_set_path)\n",
        "\n",
        "test_set_path = './vaq2.0.TestImages.txt'\n",
        "test_data = load_data(test_set_path)"
      ],
      "metadata": {
        "id": "6yu2wTKJYf66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng = spacy.load(\"en_core_web_sm\")  # Load the English model to tokenize English text\n",
        "\n",
        "def get_tokens(data_iter):\n",
        "    for sample in data_iter:\n",
        "        question = sample[\"question\"]\n",
        "        yield [token.text for token in eng.tokenizer(question)]\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    get_tokens(train_data),\n",
        "    min_freq=2,\n",
        "    specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"],\n",
        "    special_first=True\n",
        ")\n",
        "\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "suah4OL8kyhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = set([sample['answer'] for sample in train_data])\n",
        "classes_to_idx = {\n",
        "    class_name: idx for idx, class_name in enumerate(classes)\n",
        "}\n",
        "\n",
        "idx_to_classes = {\n",
        "    idx: class_name for class_name, idx in enumerate(classes)\n",
        "}"
      ],
      "metadata": {
        "id": "P5OCCHa1Yf7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(quesion, max_sequence_length):\n",
        "    tokens = [token.text for token in eng.tokenizer(quesion)]\n",
        "    sequence = [vocab[token] for token in tokens]\n",
        "    if len(sequence) < max_sequence_length:\n",
        "        sequence += [vocab[\"<pad>\"]] * (max_sequence_length - len(sequence))\n",
        "    else:\n",
        "        sequence = sequence[:max_sequence_length]\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "Wyv60SUYkpjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(self, data, classes_to_idx, max_seq_len=30, transform=None, root_dir='val2014-resided'):\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.root_dir = root_dir\n",
        "        self.classes_to_idx = classes_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data[idx]['image_path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        question = self.data[idx]['question']\n",
        "        question = tokenize(question, self.max_seq_len)\n",
        "        question = torch.tensor(question, dtype=torch.long)\n",
        "\n",
        "        answer = self.data[idx]['answer']\n",
        "        answer = self.classes_to_idx[answer]\n",
        "        answer = torch.tensor(answer, dtype=torch.long)\n",
        "\n",
        "        return image, question, answer"
      ],
      "metadata": {
        "id": "CdJSlf_ZivJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "SJ-Xk7FtivJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = VQADataset(train_data, classes_to_idx, transform=transform)\n",
        "val_dataset = VQADataset(val_data, classes_to_idx, transform=transform)\n",
        "test_dataset = VQADataset(test_data, classes_to_idx, transform=transform)"
      ],
      "metadata": {
        "id": "UkL8wsuLivJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 128\n",
        "test_batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "yQnQrvkHivJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(self, data, classes_to_idx, img_feature_extractor, text_tokenizer, device, root_dir='./val2014-resided'):\n",
        "        self.data = data\n",
        "        self.root_dir = root_dir\n",
        "        self.classes_to_idx = classes_to_idx\n",
        "        self.img_feature_extractor = img_feature_extractor\n",
        "        self.text_tokenizer = text_tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.root_dir, self.data[index]['image_path'])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.img_feature_extractor:\n",
        "            img = self.img_feature_extractor(img, return_tensors='pt')\n",
        "            img = {k: v.to(self.device) for k, v in img.items()}\n",
        "\n",
        "        question = self.data[index]['question']\n",
        "        if self.text_tokenizer:\n",
        "            question = self.text_tokenizer(\n",
        "                question,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=30,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            question = {k: v.to(self.device) for k, v in question.items()}\n",
        "\n",
        "        label = self.data[index]['answer']\n",
        "        label = torch.tensor(self.classes_to_idx[label], dtype=torch.long).to(device)\n",
        "        sample = {\n",
        "            'image': img,\n",
        "            'question': question,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "BDwU6sT3YpEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_dataset = VQADataset(\n",
        "    train_data,\n",
        "    classes_to_idx=classes_to_idx,\n",
        "    img_feature_extractor=img_feature_extractor,\n",
        "    text_tokenizer=text_tokenizer,\n",
        "    #label_encoder=label_encoder,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "val_dataset = VQADataset(\n",
        "    val_data,\n",
        "    classes_to_idx=classes_to_idx,\n",
        "    img_feature_extractor=img_feature_extractor,\n",
        "    text_tokenizer=text_tokenizer,\n",
        "    #label_encoder=label_encoder,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "test_dataset = VQADataset(\n",
        "    test_data,\n",
        "    classes_to_idx=classes_to_idx,\n",
        "    img_feature_extractor=img_feature_extractor,\n",
        "    text_tokenizer=text_tokenizer,\n",
        "    #label_encoder=label_encoder,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "MpSklYWIZTpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs.pooler_output\n",
        "\n",
        "class VisualEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisualEncoder, self).__init__()\n",
        "        self.model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs.pooler_output"
      ],
      "metadata": {
        "id": "wEQ59E4zakKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size=768*2, hidden_size=512, n_layers=1, dropout=0.2,  n_classes=2):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size*2, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "LzH47CH9azWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, text_encoder, visual_encoder, classifier):\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.text_encoder = text_encoder\n",
        "        self.visual_encoder = visual_encoder\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def forward(self, x):\n",
        "        text_out = self.text_encoder(x['question'])\n",
        "        img_out = self.visual_encoder(x['image'])\n",
        "        x = torch.cat((text_out, img_out), dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def freeze(self, visual=True, textual=True, clas=False):\n",
        "        if visual:\n",
        "            for n, p in self.visual_encoder.named_parameters():\n",
        "                p.requires_grad = False\n",
        "        if textual:\n",
        "            for n, p in self.text_encoder.named_parameters():\n",
        "                p.requires_grad = False\n",
        "        if clas:\n",
        "            for n, p in self.classifier.named_parameters():\n",
        "                p.requires_grad = False"
      ],
      "metadata": {
        "id": "gUnihLNPckk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(classes)\n",
        "hidden_size = 1024\n",
        "n_layers = 1\n",
        "dropout_prob = 0.2\n",
        "\n",
        "text_encoder = TextEncoder().to(device)\n",
        "visual_encoder = VisualEncoder().to(device)\n",
        "\n",
        "classifier = Classifier(\n",
        "    hidden_size=hidden_size,\n",
        "    n_layers=n_layers,\n",
        "    dropout=dropout_prob,\n",
        "    n_classes=n_classes\n",
        ").to(device)\n",
        "\n",
        "model = VQAModel(\n",
        "    visual_encoder=visual_encoder,\n",
        "    text_encoder=text_encoder,\n",
        "    classifier=classifier\n",
        ").to(device)\n",
        "\n",
        "model.freeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzEgj_mleom6",
        "outputId": "ca82c0cd-7b8f-4c6b-c79d-c38dc8c19372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, inputs in enumerate(dataloader):\n",
        "            images = inputs['image']\n",
        "            questions = inputs['question']\n",
        "            labels = inputs['label']\n",
        "            outputs = model(images, questions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    loss = sum(losses) / len(losses)\n",
        "    acc = correct / total\n",
        "\n",
        "    return loss, acc"
      ],
      "metadata": {
        "id": "1_rH4qvxezCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    epochs\n",
        "):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch_train_losses = []\n",
        "\n",
        "        model.train()\n",
        "        for idx, inputs in enumerate(train_loader):\n",
        "            images = inputs['image']\n",
        "            questions = inputs['question']\n",
        "            labels = inputs['label']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images, questions)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss, val_acc = evaluate(\n",
        "            model, val_loader,\n",
        "            criterion\n",
        "        )\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'EPOCH {epoch + 1}:\\\\tTrain loss: {train_loss: .4f}\\\\tVal loss: {val_loss: .4f}\\\\tVal Acc: {val_acc}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "ZBiusfCrfFSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "epochs = 50\n",
        "scheduler_step_size = epochs * 0.6\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=lr\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=scheduler_step_size,\n",
        "    gamma=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "PsjNcojBfjU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss = fit(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    epochs\n",
        ")"
      ],
      "metadata": {
        "id": "kdQT3jx1fst3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = evaluate(model, val_dataloader, criterion)\n",
        "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
        "\n",
        "print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "z0SkXJgcfzrC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iTFGes9iDYmAD28z4gGg7xpxCZ8Sl0nL",
      "authorship_tag": "ABX9TyPuDYWLfr2SHyuMy02vTBh6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}